{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('data_analy': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3ddee81a1b27f05db421b9f5cb6fc46ca6f9935fcf31d769e2300ba44e2b6654"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capter8 提升方法\n",
    "## Adaboost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class AdaBoost():\n",
    "    def __init__(self, G_num = 5):\n",
    "        self.G_num = G_num\n",
    "\n",
    "    #使用LR作为基学习器\n",
    "    def G(self, train_x, train_y, Dw, epoch, lr):\n",
    "        loss_lst = []\n",
    "        w = np.zeros(train_x.shape[1])\n",
    "        b = 0\n",
    "        for i in range(epoch):\n",
    "            h = 1 / (1 + np.exp(-1 * (np.dot(train_x, w.T) + b)))\n",
    "            L = -1 * np.dot((train_y * np.log(h) + (1 - train_y) * np.log(1 - h)).T, Dw)\n",
    "            loss_lst.append(L)\n",
    "            dL_dw = np.dot(((h - train_y) * Dw).T, train_x)\n",
    "            dL_db = np.dot((h - train_y).T, Dw)\n",
    "            w = w - lr * dL_dw\n",
    "            b = b - lr * dL_db\n",
    "        print(\"G loss is \" + ' '.join([str(round(x,2)) for x in loss_lst]))\n",
    "        return (w, b)\n",
    "\n",
    "    def G_pre(self, x, w, b):\n",
    "        pre = 1 / (1 + np.exp(-1 * (np.dot(x, w.T) + b)))\n",
    "        return np.where(pre<=0.5, -1, 1)\n",
    "\n",
    "    def fit(self, train_x, train_y, G_epoch=10, lr = 0.05):\n",
    "        self.G_lst = []\n",
    "        step_loss = []\n",
    "        n = train_x.shape[0]\n",
    "        Dw = np.zeros(n) + 1/n\n",
    "        for i in range(self.G_num):\n",
    "            w, b = self.G(train_x, train_y, Dw, G_epoch, lr)\n",
    "            pre = self.G_pre(train_x, w, b)\n",
    "            err_idx = np.nonzero(pre-train_y)[0]\n",
    "            e = np.sum(Dw[err_idx])\n",
    "            step_loss.append(str(len(err_idx)))\n",
    "            a = 1/2 * np.log((1-e)/e)\n",
    "            self.G_lst.append({'a': a, 'w' : w, 'b' : b})\n",
    "            Dw_raw = Dw * np.exp(-1*a*train_y*pre)\n",
    "            Dw = Dw_raw/sum(Dw_raw)\n",
    "        print('%d model error sample num is %s' % (self.G_num, ' '.join(step_loss)))\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        val = np.zeros(test_x.shape[0])\n",
    "        for arg in self.G_lst:\n",
    "            val = val + arg['a'] * self.G_pre(test_x, arg['w'], arg['b'])\n",
    "        return np.where(val>0, 1, -1)\n",
    "\n",
    "    def get_accuracy(self, gt_y, pre_y):\n",
    "        diff = np.subtract(gt_y, pre_y)\n",
    "        err = diff[diff!=0].shape[0]\n",
    "        return 1 - err/gt_y.shape[0]\n",
    "\n",
    "    def plot_decision_boundary(self, fea, label, ax, title=''):\n",
    "        fea1_min, fea1_max = fea[:, 0].min() - 0.5, fea[:, 0].max() + 0.5\n",
    "        fea2_min, fea2_max = fea[:, 1].min() - 0.5, fea[:, 1].max() + 0.5\n",
    "        s = 0.05\n",
    "        g_fea1, g_fea2 = np.meshgrid(np.arange(fea1_min, fea1_max, s), np.arange(fea2_min, fea2_max, s))\n",
    "        pre_y = self.predict(np.c_[g_fea1.ravel(), g_fea2.ravel()])\n",
    "        pre_y = pre_y.reshape(g_fea1.shape)\n",
    "        ax.contourf(g_fea1, g_fea2, pre_y, cmap=plt.cm.Spectral)\n",
    "        ax.scatter(fea[:, 0], fea[:, 1], c = label, s = 20)\n",
    "        if len(title) > 0:\n",
    "            ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "G loss is 0.69 0.63 0.56 0.5 0.45 0.39 0.34 0.29 0.24 0.19\nG loss is 0.69 0.68 0.66 0.64 0.62 0.61 0.59 0.58 0.56 0.55\nG loss is 0.69 0.68 0.68 0.67 0.66 0.65 0.65 0.64 0.63 0.62\nG loss is 0.69 0.69 0.68 0.67 0.66 0.66 0.65 0.64 0.64 0.63\nG loss is 0.69 0.69 0.68 0.67 0.67 0.66 0.66 0.65 0.65 0.64\n5 model error sample num is 20 16 19 20 20\naccuracy: 0.73\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ce29e22806747e19ff65049fb32aa85"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from sklearn import datasets\n",
    "sample_num = 200\n",
    "inputs, labels = datasets.make_classification(n_samples=sample_num, n_features=2, n_classes=2, n_redundant=0)\n",
    "labels = np.where(labels == 0, -1, labels)\n",
    "train_num = sample_num//2\n",
    "train_x = inputs[:train_num]\n",
    "train_y = labels[:train_num]\n",
    "test_x = inputs[train_num:]\n",
    "test_y = labels[train_num:]\n",
    "\n",
    "model = AdaBoost(5)\n",
    "model.fit(train_x, train_y)\n",
    "pre_y = model.predict(test_x)\n",
    "print('accuracy: %.2f' % model.get_accuracy(test_y, pre_y))\n",
    "model.plot_decision_boundary(train_x, train_y, plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}